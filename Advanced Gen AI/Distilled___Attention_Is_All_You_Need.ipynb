{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ§  Distilled | Attention Is All You Need\n",
        "###### <span style=\"font-size:14pt\">A Simplified Guide to Transformers and Attention Mechanisms</span> [paper link](https://arxiv.org/abs/1706.03762)"
      ],
      "metadata": {
        "id": "e2Xx4YsjVKLI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ“Œ Introduction\n",
        "\n",
        "The Transformer architecture, introduced in the 2017 paper \"Attention Is All You Need\", has become the foundation of modern natural language processing. Unlike previous models that relied on recurrence (RNNs, LSTMs) or convolutions (CNNs), Transformers rely solely on **attention mechanisms** to model relationships in data.\n",
        "\n",
        "This notebook provides a **distilled** explanation of the core attention concepts that power Transformers. My goal is to make these ideas accessible by combining intuitive explanations with practical code examples in PyTorch.\n",
        "\n",
        "By the end of this notebook, you'll understand:\n",
        "\n",
        "- What attention is and why it's powerful  \n",
        "- The difference between **self-attention**, **multi-head attention**, **causal attention**, and **cross-attention**  \n",
        "- How to implement each type of attention in code  \n",
        "- How these pieces come together in the Transformer architecture  \n",
        "\n",
        "Whether you're a student, researcher, or developer, this notebook is designed to be a clear, hands-on starting point for understanding how attention works in modern AI."
      ],
      "metadata": {
        "id": "2fndbu1ZWth8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To properly format your content in a **Text cell** in Google Colab, including the equation, you can use Markdown combined with LaTeX syntax. Here's how you can do it:\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ” What is Attention?\n",
        "\n",
        "At its core, **attention** is a mechanism that allows a model to dynamically focus on different parts of its input when performing a task â€” such as translating a sentence or answering a question.\n",
        "\n",
        "Imagine reading a sentence and trying to understand the meaning of a word like \"it\". To figure out what \"it\" refers to, your brain \"attends\" to earlier parts of the sentence. Attention mechanisms allow neural networks to do the same â€” to look at other words in the input and weigh their importance dynamically.\n",
        "\n",
        "### ðŸ“ The Scaled Dot-Product Attention\n",
        "\n",
        "The most common form of attention used in Transformers is called **scaled dot-product attention**. It operates on three vectors:\n",
        "\n",
        "- **Q (Query)** â€“ What weâ€™re looking for  \n",
        "- **K (Key)** â€“ What we compare it to  \n",
        "- **V (Value)** â€“ What we retrieve if it matches  \n",
        "\n",
        "The formula is:\n",
        "\n",
        "$$\n",
        "\\text{Attention}(Q, K, V) = \\text{softmax}\\left( \\frac{QK^T}{\\sqrt{d_k}} \\right)V\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "- \\( Q \\), \\( K \\), and \\( V \\) are matrices representing the query, key, and value vectors.  \n",
        "- \\( d_k \\) is the dimension of the key vectors, used to scale the dot products.  \n",
        "- The **softmax** ensures that the attention weights are positive and sum to 1.\n",
        "\n",
        "### ðŸ§  Intuition\n",
        "\n",
        "- The **dot product** \\( $$QK^T$$ \\) tells us how similar the query is to each key.  \n",
        "- Dividing by \\( $$\\sqrt{d_k}$$ \\) prevents extremely large values (which can harm learning).  \n",
        "- The **softmax** turns these similarities into a probability distribution â€” essentially \"how much attention\" to pay to each position.  \n",
        "- Finally, we apply these weights to the **values (V)** to get the output â€” a weighted combination of input information.\n",
        "\n",
        "This core idea is used throughout the Transformer â€” and extended in various ways to support richer behavior like multi-head, causal, and cross attention.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "kQsCRZ3VXYKd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸŽ¯ Types of Attention\n",
        "\n",
        "In this section, we'll explore the four key types of attention used in the Transformer architecture:\n",
        "\n",
        "- **Self-Attention**\n",
        "- **Multi-Head Attention**\n",
        "- **Causal Attention**\n",
        "- **Cross Attention**\n",
        "\n",
        "Each attention type builds on the same underlying mechanism but serves different purposes within the model. We'll explain each one and provide a PyTorch implementation."
      ],
      "metadata": {
        "id": "Cj5UiXgQaRf5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸ§© Self-Attention\n",
        "\n",
        "**Self-Attention** allows a sequence element (e.g., a word) to attend to **other elements in the same sequence** to build context. For example, to understand the meaning of \"bank\", we may need to attend to \"river\" or \"money\" nearby.\n",
        "\n",
        "In Transformers, each token in the input is compared with every other token (including itself), and a weighted representation is formed.\n",
        "\n",
        "This is the foundation of both encoder and decoder blocks.\n"
      ],
      "metadata": {
        "id": "AunkmXplabIM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "Q = torch.rand((3, 4))\n",
        "K = torch.rand((3, 4))\n",
        "V = torch.rand((3, 4))\n",
        "\n",
        "dk = Q.size(-1)\n",
        "attention_scores = Q @ K.T / dk**0.5\n",
        "attention_weights = F.softmax(attention_scores, dim=-1)\n",
        "output = attention_weights @ V\n",
        "\n",
        "print(\"Attention Weights:\\n\", attention_weights)\n",
        "print(\"\\nSelf-Attention Output:\\n\", output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOn-7o5vWrlD",
        "outputId": "3928d150-d84f-424d-92ca-418501e3c507"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention Weights:\n",
            " tensor([[0.3237, 0.3267, 0.3496],\n",
            "        [0.3015, 0.3176, 0.3809],\n",
            "        [0.2737, 0.3728, 0.3534]])\n",
            "\n",
            "Self-Attention Output:\n",
            " tensor([[0.3456, 0.1623, 0.3909, 0.7038],\n",
            "        [0.3319, 0.1635, 0.4014, 0.7107],\n",
            "        [0.3484, 0.1527, 0.3724, 0.6964]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸ” Multi-Head Attention\n",
        "\n",
        "**Multi-Head Attention** allows the model to learn **multiple attention distributions** in parallel. Each \"head\" focuses on different parts or relationships in the input.\n",
        "\n",
        "The results from all heads are concatenated and projected to the output space.\n",
        "\n",
        "This is a core component of all Transformer layers.\n"
      ],
      "metadata": {
        "id": "bZLscfk5a7Zc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Input: 3 tokens, 8-dimensional embeddings\n",
        "x = torch.rand((3, 8))\n",
        "\n",
        "# Parameters\n",
        "num_heads = 2\n",
        "d_model = x.size(-1)\n",
        "d_k = d_model // num_heads\n",
        "\n",
        "# Randomly initialized projection weights for Q, K, V for each head\n",
        "W_q = torch.rand((num_heads, d_model, d_k))\n",
        "W_k = torch.rand((num_heads, d_model, d_k))\n",
        "W_v = torch.rand((num_heads, d_model, d_k))\n",
        "W_o = torch.rand((num_heads * d_k, d_model))  # final output projection\n",
        "\n",
        "# Step 1: Project input into Q, K, V for each head\n",
        "Q = torch.einsum('nd, hdf -> hnf', x, W_q)  # shape: (heads, tokens, d_k)\n",
        "K = torch.einsum('nd, hdf -> hnf', x, W_k)\n",
        "V = torch.einsum('nd, hdf -> hnf', x, W_v)\n",
        "\n",
        "# Step 2: Compute scaled dot-product attention per head\n",
        "scores = torch.einsum('hnf, hmf -> hnm', Q, K) / (d_k ** 0.5)  # (heads, tokens, tokens)\n",
        "weights = F.softmax(scores, dim=-1)\n",
        "attn_output = torch.einsum('hnm, hmf -> hnf', weights, V)  # (heads, tokens, d_k)\n",
        "\n",
        "# Step 3: Concatenate heads\n",
        "attn_output = attn_output.transpose(0, 1).reshape(3, -1)  # (tokens, heads * d_k)\n",
        "\n",
        "# Step 4: Final linear projection\n",
        "final_output = attn_output @ W_o  # (tokens, d_model)\n",
        "\n",
        "print(\"Multi-Head Attention Output:\\n\", final_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ane6y_viVgyg",
        "outputId": "5d3b2b66-7d68-4ded-d8cd-6ebdf6294d9d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Multi-Head Attention Output:\n",
            " tensor([[10.5482,  9.1351,  7.7318,  9.3621,  9.1614, 10.2630,  5.0940,  7.4390],\n",
            "        [10.6146,  9.2001,  7.7724,  9.4230,  9.2220, 10.3311,  5.1205,  7.4808],\n",
            "        [10.4710,  9.0539,  7.6865,  9.2882,  9.0836, 10.1739,  5.0604,  7.3916]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸš« Causal (Masked) Attention\n",
        "\n",
        "**Causal Attention** prevents tokens from attending to **future positions** in the sequence. It's used in language models like GPT to ensure that prediction at position `t` only depends on positions â‰¤ `t`.\n",
        "\n",
        "This is achieved using a **lower-triangular mask**."
      ],
      "metadata": {
        "id": "V6K5gSBwbBBZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Input: batch_size=1, 3 tokens, 8-dimensional embeddings\n",
        "x = torch.rand((1, 3, 8))  # (batch_size, seq_len, d_model)\n",
        "\n",
        "# Parameters\n",
        "num_heads = 2\n",
        "d_model = x.size(-1)\n",
        "d_k = d_model // num_heads\n",
        "\n",
        "# Randomly initialized projection weights for Q, K, V for each head\n",
        "W_q = torch.rand((num_heads, d_model, d_k))\n",
        "W_k = torch.rand((num_heads, d_model, d_k))\n",
        "W_v = torch.rand((num_heads, d_model, d_k))\n",
        "W_o = torch.rand((num_heads * d_k, d_model))  # final output projection\n",
        "\n",
        "# Step 1: Project input into Q, K, V for each head\n",
        "Q = torch.einsum('bnd, hdf -> bhnf', x, W_q)  # shape: (batch_size, heads, seq_len, d_k)\n",
        "K = torch.einsum('bnd, hdf -> bhnf', x, W_k)\n",
        "V = torch.einsum('bnd, hdf -> bhnf', x, W_v)\n",
        "\n",
        "# Step 2: Create causal mask (lower triangular matrix) for each sequence in the batch\n",
        "seq_len = x.size(1)\n",
        "mask = torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0).unsqueeze(1)  # (1, 1, seq_len, seq_len)\n",
        "\n",
        "# Step 3: Compute scaled dot-product attention per head with causal mask\n",
        "scores = torch.einsum('bhnf, bhmf -> bhnm', Q, K) / (d_k ** 0.5)  # (batch_size, heads, seq_len, seq_len)\n",
        "scores = scores.masked_fill(mask == 0, float('-inf'))  # Apply causal mask\n",
        "weights = F.softmax(scores, dim=-1)\n",
        "attn_output = torch.einsum('bhnm, bhmf -> bhnf', weights, V)  # (batch_size, heads, seq_len, d_k)\n",
        "\n",
        "# Step 4: Concatenate heads (from heads dimension)\n",
        "attn_output = attn_output.transpose(1, 2).contiguous().view(1, seq_len, -1)  # (batch_size, seq_len, heads * d_k)\n",
        "\n",
        "# Step 5: Final linear projection\n",
        "final_output = attn_output @ W_o  # (batch_size, seq_len, d_model)\n",
        "\n",
        "print(\"Causal Multi-Head Attention Output:\\n\", final_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBi-1hVPbCAq",
        "outputId": "4173255c-6f4e-4e06-f135-ab1883a47ba1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Causal Multi-Head Attention Output:\n",
            " tensor([[[ 6.6643,  9.4161, 10.0614,  8.9188, 11.8910,  8.9163, 12.2942,\n",
            "          12.1606],\n",
            "         [ 6.4994,  9.0797,  9.5428,  8.6220, 11.6023,  8.6823, 11.8995,\n",
            "          11.7685],\n",
            "         [ 6.2610,  8.9364,  9.2423,  8.4325, 11.2500,  8.5664, 11.6209,\n",
            "          11.5014]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸ”„ Cross Attention\n",
        "\n",
        "**Cross Attention** is used in **encoder-decoder** models, where the decoder input (query) attends to the encoder output (key and value).\n",
        "\n",
        "It's the key attention type in applications like machine translation or image captioning."
      ],
      "metadata": {
        "id": "lZt2iv87bMEe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Input: batch_size=1, 3 tokens in encoder and decoder, 8-dimensional embeddings\n",
        "encoder_input = torch.rand((1, 3, 8))  # (batch_size, seq_len, d_model)\n",
        "decoder_input = torch.rand((1, 3, 8))  # (batch_size, seq_len, d_model)\n",
        "\n",
        "# Parameters\n",
        "num_heads = 2\n",
        "d_model = encoder_input.size(-1)\n",
        "d_k = d_model // num_heads\n",
        "\n",
        "# Randomly initialized projection weights for Q, K, V for each head\n",
        "W_q = torch.rand((num_heads, d_model, d_k))\n",
        "W_k = torch.rand((num_heads, d_model, d_k))\n",
        "W_v = torch.rand((num_heads, d_model, d_k))\n",
        "W_o = torch.rand((num_heads * d_k, d_model))  # final output projection\n",
        "\n",
        "# Step 1: Project encoder and decoder input into Q, K, V for each head\n",
        "Q = torch.einsum('bnd, hdf -> bhnf', decoder_input, W_q)  # shape: (batch_size, heads, seq_len, d_k)\n",
        "K = torch.einsum('bnd, hdf -> bhnf', encoder_input, W_k)\n",
        "V = torch.einsum('bnd, hdf -> bhnf', encoder_input, W_v)\n",
        "\n",
        "# Step 2: Create causal mask (lower triangular matrix) for each sequence in the decoder\n",
        "seq_len = decoder_input.size(1)\n",
        "mask = torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0).unsqueeze(1)  # (1, 1, seq_len, seq_len)\n",
        "\n",
        "# Step 3: Compute scaled dot-product attention per head with causal mask\n",
        "scores = torch.einsum('bhnf, bhmf -> bhnm', Q, K) / (d_k ** 0.5)  # (batch_size, heads, seq_len, seq_len)\n",
        "scores = scores.masked_fill(mask == 0, float('-inf'))  # Apply causal mask\n",
        "weights = F.softmax(scores, dim=-1)\n",
        "attn_output = torch.einsum('bhnm, bhmf -> bhnf', weights, V)  # (batch_size, heads, seq_len, d_k)\n",
        "\n",
        "# Step 4: Concatenate heads (from heads dimension)\n",
        "attn_output = attn_output.transpose(1, 2).contiguous().view(1, seq_len, -1)  # (batch_size, seq_len, heads * d_k)\n",
        "\n",
        "# Step 5: Final linear projection\n",
        "final_output = attn_output @ W_o  # (batch_size, seq_len, d_model)\n",
        "\n",
        "print(\"Cross Attention Output:\\n\", final_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOPSgQQubOp9",
        "outputId": "b42cf4f3-d5d2-4f4b-efc1-5d2aad3265f4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross Attention Output:\n",
            " tensor([[[ 9.2096,  9.4300,  5.6639,  4.4452,  5.7366,  7.6541,  6.8250,\n",
            "           6.9600],\n",
            "         [ 9.8741, 10.0145,  6.1487,  5.0298,  6.2863,  7.9965,  7.7780,\n",
            "           7.3250],\n",
            "         [10.0505, 10.1721,  6.2643,  5.1377,  6.4207,  8.1500,  7.9149,\n",
            "           7.4401]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusion\n",
        "\n",
        "In this section, weâ€™ve explored key attention mechanisms that power **Transformer** models: **Self-Attention**, **Multi-Head Attention**, **Causal Attention**, and **Cross-Attention**. Here's a simplified summary:\n",
        "\n",
        "- **Self-Attention:** Each token in a sequence attends to every other token in the same sequence to gather contextual information. This is essential for understanding relationships in the data.\n",
        "  \n",
        "- **Multi-Head Attention:** Multiple attention heads allow the model to focus on different aspects of the sequence in parallel, providing richer information.\n",
        "\n",
        "- **Causal Attention (Masked Attention):** In autoregressive models like GPT, tokens can only attend to themselves and previous tokens to prevent information leakage from future tokens during training.\n",
        "\n",
        "- **Cross-Attention:** In encoder-decoder models (like machine translation), the decoder attends to the encoderâ€™s output, focusing on relevant parts of the input sequence.\n",
        "\n",
        "Together, these mechanisms enable Transformers to handle complex tasks like **machine translation**, **text generation**, and **image captioning**, making them incredibly powerful for sequence-based tasks.\n"
      ],
      "metadata": {
        "id": "GTbBTN8Ne36n"
      }
    }
  ]
}